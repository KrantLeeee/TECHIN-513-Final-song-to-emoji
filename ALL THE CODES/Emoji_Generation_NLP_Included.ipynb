{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting numpy>=1.17 (from accelerate)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.8/114.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/krantlee/Library/Python/3.11/lib/python/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /Users/krantlee/Library/Python/3.11/lib/python/site-packages (from accelerate) (5.9.8)\n",
      "Collecting pyyaml (from accelerate)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Downloading torch-2.2.1-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting huggingface-hub (from accelerate)\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate)\n",
      "  Downloading safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from torch>=1.10.0->accelerate)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch>=1.10.0->accelerate)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch>=1.10.0->accelerate)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.10.0->accelerate)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.10.0->accelerate)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch>=1.10.0->accelerate)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting requests (from huggingface-hub->accelerate)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub->accelerate)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch>=1.10.0->accelerate)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub->accelerate)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub->accelerate)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub->accelerate)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub->accelerate)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.10.0->accelerate)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl (393 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.1-cp311-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.5/167.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl (18 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, safetensors, pyyaml, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, huggingface-hub, accelerate\n",
      "Successfully installed MarkupSafe-2.1.5 accelerate-0.27.2 certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2024.2.0 huggingface-hub-0.21.3 idna-3.6 jinja2-3.1.3 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.4 pyyaml-6.0.1 requests-2.31.0 safetensors-0.4.2 sympy-1.12 torch-2.2.1 tqdm-4.66.2 typing-extensions-4.10.0 urllib3-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/krantlee/Library/Python/3.11/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tokenizers, transformers\n",
      "Successfully installed regex-2023.12.25 tokenizers-0.15.2 transformers-4.38.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate -U\n",
    "\n",
    "%pip install torch -U\n",
    "%pip install transformers -U\n",
    "\n",
    "pip install pandas scikit-learn numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train The NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__\n",
    "\n",
    "zip_path = '' # Provide the Emotion-Lexicon Dataset path name\n",
    "extract_to = ''  # Provide the Emotion-Lexicon Extraction path name\n",
    "with ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "file_path = '/content/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'  # Replace the extraction path name with yours\n",
    "emotion_lexicon = pd.read_csv(file_path, names=[\"word\", \"emotion\", \"association\"], sep='\\t')\n",
    "\n",
    "# Pivot the table to have one row per word and emotion columns\n",
    "emotion_data = emotion_lexicon.pivot(index=\"word\", columns=\"emotion\", values=\"association\").reset_index()\n",
    "\n",
    "print(emotion_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the required emotion catagories\n",
    "emotions = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"trust\"]\n",
    "emotion_data_filtered = emotion_data[[\"word\"] + emotions]\n",
    "print(emotion_data_filtered.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Extract labels from the DataFrame\n",
    "labels = emotion_data_filtered[emotions].values\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # Thresholding at 0.5 to decide on label presence\n",
    "    preds = pred.predictions >= 0.5\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # Calculate F1 scores\n",
    "    f1_micro = f1_score(labels, preds, average='micro')\n",
    "    f1_macro = f1_score(labels, preds, average='macro')\n",
    "\n",
    "    # Returning multiple metrics\n",
    "    return {\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "    }\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "        # Ensure the lengths match\n",
    "        assert len(self.encodings['input_ids']) == len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure idx is within the range\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Index out of range in EmotionDataset\")\n",
    "\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize all texts\n",
    "# Filter NaN rows\n",
    "emotion_data_filtered = emotion_data_filtered.dropna(subset=['word']).reset_index(drop=True)\n",
    "\n",
    "labels = emotion_data_filtered[emotions].values\n",
    "\n",
    "texts = emotion_data_filtered['word'].dropna().astype(str).tolist()\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Ensure the lengths of encodings and labels are the same\n",
    "assert len(encodings['input_ids']) == len(labels), \"The lengths of encodings and labels do not match after filtering.\"\n",
    "print(f\"Length of encodings: {len(encodings['input_ids'])}\")\n",
    "print(f\"Length of labels: {len(labels)}\")\n",
    "\n",
    "# Create the dataset\n",
    "dataset = EmotionDataset(encodings, labels)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Check lengths before training\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "try:\n",
    "    # Try to retrieve the first item as a check\n",
    "    first_train_item = train_dataset[0]\n",
    "    first_val_item = val_dataset[0]\n",
    "except IndexError as e:\n",
    "    print(\"An error occurred when retrieving an item from the dataset:\", e)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(emotions))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics  # Define your compute_metrics function for accuracy, etc.\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('') # Add a save path to save the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji Generation Based on Emotion Analysis by NLP Model â¬‡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger: 0.0782\n",
      "anticipation: 0.1479\n",
      "disgust: 0.0504\n",
      "fear: 0.1721\n",
      "joy: 0.0615\n",
      "sadness: 0.1605\n",
      "surprise: 0.1051\n",
      "trust: 0.0163\n",
      "[0.0782, 0.1479, 0.0504, 0.1721, 0.0615, 0.1605, 0.1051, 0.0163]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# The folder path where the model and weights are saved\n",
    "model_path = '.../V1 Lyrics_to_emoji'\n",
    "\n",
    "# Load the model\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Make sure you also use the correct tokenizer, here use the same tokenizer as during training\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "\n",
    "text = '''There's a fire starting in my heart\n",
    "Reaching a fever pitch, it's bringing me out the dark\n",
    "Finally I can see you crystal clear\n",
    "Go ahead and sell me out and I'll lay your ship bare\n",
    "See how I'll leave with every piece of you\n",
    "Don't underestimate the things that I will do\n",
    "There's a fire starting in my heart\n",
    "Reaching a fever pitch and it's bringing me out the dark\n",
    "The scars of your love remind me of us\n",
    "They keep me thinking that we almost had it all\n",
    "The scars of your love they leave me breathless\n",
    "I can't help feeling\n",
    "We could've had it all (you're gonna wish you)\n",
    "(Never had met me)\n",
    "Rolling in the deep (tears are gonna fall)\n",
    "(Rolling in the deep)\n",
    "You had my heart inside (you're gonna wish you)\n",
    "Of your hands (never had met me)\n",
    "And you played it (tears are gonna fall)\n",
    "To the beat (rolling in the deep)\n",
    "Baby, I have no story to be told\n",
    "But I've heard one on you, now I'm gonna make your head burn\n",
    "Think of me in the depths of your despair\n",
    "Make a home down there, as mine sure won't be shared\n",
    "The scars of your love (never had met me)\n",
    "Remind me of us (tears are gonna fall)\n",
    "They keep me thinking (rolling in the deep)\n",
    "That we almost had it all (you're gonna wish you)\n",
    "The scars of your love (never had met me)\n",
    "They leave me breathless (tears are gonna fall)\n",
    "I can't help feeling (rolling in the deep)\n",
    "We could've had it all (you're gonna wish you)\n",
    "(Never had met me)\n",
    "Rolling in the deep (tears are gonna fall)\n",
    "(Rolling in the deep)\n",
    "You had my heart inside (you're gonna wish you)\n",
    "Of your hands (never had met me)\n",
    "And you played it (tears are gonna fall)\n",
    "To the beat (rolling in the deep)\n",
    "We could've had it all\n",
    "Rolling in the deep\n",
    "You had my heart inside of your hand\n",
    "But you played it with a beating\n",
    "Throw your soul through every open door (whoa)\n",
    "Count your blessings to find what you look for (whoa)\n",
    "Turn my sorrow into treasured gold (whoa)\n",
    "You pay me back in kind and reap just what you've sown\n",
    "We could've had it all (tears are gonna fall, rolling in the deep)\n",
    "We could've had it all (you're gonna wish you never had met me)\n",
    "It all, it all, it all (tears are gonna fall, rolling in the deep)\n",
    "We could've had it all (you're gonna wish you)\n",
    "(Never had met me)\n",
    "Rolling in the deep (tears are gonna fall)\n",
    "(Rolling in the deep)\n",
    "You had my heart inside (you're gonna wish you)\n",
    "Of your hands (never had met me)\n",
    "And you played it (tears are gonna fall)\n",
    "To the beat (rolling in the deep)\n",
    "Could've had it all (you're gonna wish you)\n",
    "(Never had met me)\n",
    "Rolling in the deep (tears are gonna fall)\n",
    "(Rolling in the deep)\n",
    "You had my heart inside (you're gonna wish you)\n",
    "Of your hands (never had met me)\n",
    "But you played it, you played it, you played it\n",
    "You played it to the beat\n",
    "\n",
    "\n",
    "'''\n",
    "inputs = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.sigmoid(logits).numpy()  # Apply sigmoid and convert to numpy\n",
    "\n",
    "threshold = 0.5\n",
    "predicted_labels = (predictions > threshold).astype(int)\n",
    "\n",
    "emotions_output = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"trust\"]\n",
    "\n",
    "# Print the probability of each emotion dimension\n",
    "for i, emotion1 in enumerate(emotions_output):\n",
    "    print(f\"{emotion1}: {predictions[0][i].item():.4f}\")\n",
    "\n",
    "predictions_list = []\n",
    "\n",
    "for i in range(predictions.shape[1]): \n",
    "    predictions_list.append(float(f\"{predictions[0][i].item():.4f}\"))\n",
    "\n",
    "# Print a list of converted predicted probabilities\n",
    "print(predictions_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜“: 0.9359966768247223\n",
      "ğŸ˜Ÿ: 0.9329072697488164\n",
      "ğŸ˜¨: 0.9312215434214443\n",
      "ğŸ˜•: 0.9268771173116092\n",
      "ğŸ˜³: 0.8997348964279756\n"
     ]
    }
   ],
   "source": [
    "# Use the function to generate emoji\n",
    "emotionVectorToEmoji(predictions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Is for Emoji Generation Based on Keywords Analysis by NLP Model â¬‡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ğŸ’Œ\n",
      " ğŸ©\n",
      " ğŸ‘©â€â¤ï¸â€ğŸ‘©\n",
      " ğŸ‘©â€â¤ï¸â€ğŸ‘¨\n",
      " ğŸ‘¨â€â¤ï¸â€ğŸ‘¨\n"
     ]
    }
   ],
   "source": [
    "# given lyrics, create embeddings, search for similar embeddings in the pinecone index, return the emoji unicode\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "\n",
    "# convert unicode to emoji\n",
    "def convert_unicode_to_emoji(unicode):\n",
    "    return ''.join([chr(int(code.split('+')[1], 16)) for code in unicode.split()])# split the unicode string into a list of codes, convert each code to an integer, and then to a character, and join the characters into a string\n",
    "\n",
    "# use openai to convert lyrics into text embeddings and search in the pinecone database for similar text\n",
    "openai_client = OpenAI(\n",
    "    api_key=\"\", # Provide your OpenAI api key\n",
    "    base_url=\"\" # Provide your base url\n",
    ")\n",
    "\n",
    "lyrics = \"\"\"\n",
    "If I should stay\n",
    "I would only be in your way\n",
    "So I'll go, but I know\n",
    "I'll think of you every step of the way\n",
    "And I will always love you\n",
    "I will always love you\n",
    "You\n",
    "My darling, you, mm, mm\n",
    "Bittersweet memories\n",
    "That is all I'm taking with me\n",
    "So goodbye, please don't cry\n",
    "We both know I'm not what you, you need\n",
    "And I will always love you\n",
    "I will always love you\n",
    "You\n",
    "I hope life treats you kind\n",
    "And I hope you have all you've dreamed of\n",
    "And I wish you joy and happiness\n",
    "But above all this, I wish you love\n",
    "And I will always love you\n",
    "I will always love you\n",
    "I will always love you\n",
    "I will always love you\n",
    "I will always love you\n",
    "I, I will always love you\n",
    "You\n",
    "Darling, I love you\n",
    "I'll always, I'll always love you\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# use OpenAI API text-embedding-3-small to get embeddings for the lyrics\n",
    "response = openai_client.embeddings.create(\n",
    "    input=lyrics,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "lyrics_vector = response.data[0].embedding\n",
    "\n",
    "# use Pinecone to search for the most relevant papers\n",
    "pc = Pinecone(api_key=\"31dd2029-69ce-4016-a5a1-39db8e325fb6\")\n",
    "index_name = \"emoji-description\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# search for the most relevant emojis\n",
    "query_results = index.query(\n",
    "    top_k=10, # top 10 most relevant papers\n",
    "    vector=lyrics_vector,\n",
    "    includeValues=True,\n",
    "    includeMetadata=True,\n",
    "    )\n",
    "\n",
    "# get the metadata (unicode) of the most relevant emoji\n",
    "unicode = [item['metadata']['unicode'] for item in query_results['matches']]\n",
    "final_unicode = []  # remove duplicates\n",
    "for i in unicode:\n",
    "    if i not in final_unicode:\n",
    "        final_unicode.append(i)\n",
    "    if len(final_unicode) == 5: # only extract top5 emojis\n",
    "        break\n",
    "\n",
    "# convert unicode to emoji\n",
    "final_emoji = [convert_unicode_to_emoji(i) for i in final_unicode]\n",
    "for i in final_emoji:\n",
    "    print(f\" {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is emotion_to_emoji function â¬‡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.16.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.3-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, pydantic-core, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.3.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3 pydantic-2.6.3 pydantic-core-2.16.3 sniffio-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pinecone-client) (2024.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pinecone-client) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pinecone-client) (4.10.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pinecone-client) (2.2.1)\n",
      "Downloading pinecone_client-3.1.0-py3-none-any.whl (210 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pinecone-client\n",
      "Successfully installed pinecone-client-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai\n",
    "%pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# load emoji emotion data\n",
    "df = pd.read_csv('.../EmoTag1200-scores.csv') # Provide the path name of EmoTag1200-scores dataset on your computer\n",
    "# convert emoji vectors to numpy array\n",
    "emoji_vectors = df[['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']].to_numpy()\n",
    "\n",
    "# convert emotion vector to emoji based on cosine similarity\n",
    "def emotionVectorToEmoji(emotionVector):\n",
    "  emotionVector = np.array(emotionVector).reshape(1, -1)\n",
    "  # calculate cosine similarity between text_vector and emoji_vectors\n",
    "  similarities = cosine_similarity(emotionVector, emoji_vectors)\n",
    "  # combine emoji and similarity into a list\n",
    "  emoji_similarities = list(zip(df['emoji'], similarities[0]))\n",
    "  # sort the list by similarity and get the top 10 most similar emojis\n",
    "  sorted_emojis = sorted(emoji_similarities, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "  for emoji, similarity in sorted_emojis:\n",
    "    print(f\"{emoji}: {similarity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
